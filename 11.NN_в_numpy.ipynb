{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ЛУ 11: Невронна мрежа в numpy\n",
    "\n",
    "В това упражнение ще програмираме наша собствена невронна мрежа. Тя ще е относително проста, но ще е напълно способна да бъде тренирана и да решава разнообразни задачи. За целта ще използваме единствено методи от библиотеката numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Създаване на нетрениран модел\n",
    "\n",
    "Първата стъпка ще е да създадем шаблонен модел. За целта първо трябва да дефинираме формата на модела си, а после да създадем произволни масиви числа за теглата (weights) и отклоненията (biases) за всеки отделен слой (layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 40), (40, 40), (40, 3)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ще създадем мрежа с четири входни променливи и две изходни\n",
    "# (за пример ще ползваме задачата за класифициране на ириси в три категории според четири признака).\n",
    "input_size = 4\n",
    "output_size = 3\n",
    "\n",
    "# Ще имаме два скрити слоя, всеки с 40 неврона.\n",
    "hidden_layer_sizes = [40, 40]\n",
    "\n",
    "# Вкарваме всичките си променливи в списък.\n",
    "all_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "\n",
    "# Създаваме набор от кортежи (tuples), които описват измеренията на всеки слой (вход и изход).\n",
    "layer_dimensions = [\n",
    "    (all_sizes[i-1], all_sizes[i])\n",
    "    for i in range(1, len(all_sizes))\n",
    "]\n",
    "\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тъй като ще използваме кода си многократно, ще дефинираме функции, които да ни помагат. Първоначалните стойности могат да са произволни (в случая с нормална дистрибуция) или просто нули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights( output_dim, input_dim, generate='randn'):\n",
    "\n",
    "    if generate == 'randn':\n",
    "        W = np.random.randn(output_dim, input_dim) * 0.1\n",
    "    \n",
    "    if generate == 'zeros':\n",
    "        W = np.zeros(shape=(output_dim, input_dim)) * 0.1\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biases(output_dim, generate='randn'):\n",
    "\n",
    "    if generate == 'randn':\n",
    "        b = np.random.randn(output_dim, 1) * 0.1\n",
    "    \n",
    "    if generate == 'zeros':\n",
    "        b = np.zeros(shape=(output_dim, 1)) * 0.1\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готови сме да създадем функция, която генерира основата на цялата ни невронна мрежа и я превръща в речник:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_layers(nn_architecture, seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, dimensions in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        params_values['W' + str(layer_idx)] = generate_weights(dimensions[1], dimensions[0])\n",
    "        params_values['b' + str(layer_idx)] = generate_biases(dimensions[1])\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': (40, 4),\n",
       " 'b1': (40, 1),\n",
       " 'W2': (40, 40),\n",
       " 'b2': (40, 1),\n",
       " 'W3': (3, 40),\n",
       " 'b3': (3, 1)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = generate_layers(nn_architecture=layer_dimensions)\n",
    "nn_shapes = {param: array.shape for param, array in nn.items()}\n",
    "nn_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изчисляване на слоеве\n",
    " Следващата стъпка е да напишем функция, която изчислява формулата за всеки неврон спроед входните данни. Формулата за изчисляване на резултатите от всички неврони е точковият продукт на теглата и активациите на входния слой плюс отклоненията. Накрая трябва да приложим активационната функция върху резутлатите от слоя и получаваме активациите. Така, имаме формулите \n",
    " ![image.png](https://miro.medium.com/v2/resize:fit:640/format:webp/1*-1x2XaJPJXR2FsP1DHG0Vg.gif), където:\n",
    "\n",
    " - i е номерът на слоя (т.е. i-1 е предходният слой);\n",
    " - z е резултатите на невроните в слоя;\n",
    " - W са теглата на слоя;\n",
    " - a е резултатът от активациите;\n",
    " - b са отклоненията;\n",
    " - g е активационната функция.\n",
    "\n",
    "За активационна функция ще използваме сигмоида, която се изчислява с формулата \n",
    "![sigmoid.png](https://www.gstatic.com/education/formulas2/553212783/en/sigmoid_function.svg), където e е числото на Ойлер, 2.71828, а деривативър ѝ е S(x)(1-S(x)). Нека първо дефинираме активацията и дериватива ѝ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега можем да напишем функция за изчисляване на даден слой, както и функция за изчисляване на цялата невронна мрежа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    A_curr = sigmoid(Z_curr)\n",
    "    return A_curr, Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, dimensions in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        # print('Layer ',layer_idx)\n",
    "\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr)\n",
    "\n",
    "        # print(A_prev)\n",
    "        \n",
    "        # memory съдържа резултатите от изчисленията на различните слоеве\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека тестваме изходния продукт на необучената ни мрежа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([5.3, 6.3, .8, 2.5]).reshape((4, 1)) # примерни входни стойности\n",
    "Y, memory = full_forward_propagation(test_X, nn, layer_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48290319],\n",
       "       [0.5512508 ],\n",
       "       [0.55008964]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виждаме, че произволно генерираните тегла и отклонения ни връщат почти еднакви стойности за дадени входни данни. Начинът, по който ще обучим мрежата си е като изчислим общата грешка (загуба) за всеки слой и нагласим параметрите му така, че по-добре да предвиждат в бъдеще. В случая ще започнем с изходния слой и ще изчислим разликата между y и ŷ, т.е. получения и желания резултат. В случая вторият ще изглежда като [0, 1, 0], [1, 0, 0] или [0, 0, 1] (фиктивни променливи, които отговарят на всяка категория цвете).\n",
    "\n",
    "Тази разлика ще получим като изчислим деривативите на всяка стъпка от предното движение на мрежата и ги запишем в речника grad_values. Ще използваме следните формули:\n",
    "\n",
    "![image.png](https://miro.medium.com/v2/resize:fit:536/format:webp/1*FZ4slpsaH_U0YYhaSRqUEQ.gif), където dA е деривативът на A и входните тежести и стойности на следващия слой, dZ е деривативът на активацията между Z и А и т.н. Този метод за оптимизация на алгоритъма се нарича снижаване на градиента (Gradient Descent).\n",
    "\n",
    "Изчисленията на този етап са най-сложни за разбиране, затова засега нека приложим формулите, а логиката им ще разберем с повече опит с невронните мрежи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dZ_curr = sigmoid_backward(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m #T е съкращение за transpose()\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "   \n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
    "    \n",
    "    for layer_idx_prev in reversed(range(len(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1 \n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последната стъпа е да нагодим съществуващите тегла и отклонения спрямо резултатите в речника grad_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx+1\n",
    "\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остава само да напишем функция, която да изпълнява многократно вече дефинираните ни функции, за да обучи напълно дадена мрежа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    \n",
    "    params_values = generate_layers(nn_architecture, 42)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека опитаме да тренираме мрежата си върху вече познатия ни набор данни ирис."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import load_dataset\n",
    "iris = load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.iloc[:,:4].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Y = pd.get_dummies(iris['species']).values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преди да въведем X и Y, трябва да ги транспонираме, за да получим правилни точкови продукти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_nn = train(\n",
    "    X.T,\n",
    "    Y.T,\n",
    "    layer_dimensions,\n",
    "    epochs = 1000,\n",
    "    learning_rate = .5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97538161e-01, 3.02238422e-03, 4.73647580e-09],\n",
       "       [9.97167462e-01, 3.52367542e-03, 5.41789218e-09],\n",
       "       [9.97335201e-01, 3.29387446e-03, 5.11572422e-09],\n",
       "       [9.96960699e-01, 3.79735165e-03, 5.79828928e-09],\n",
       "       [9.97557441e-01, 2.99483218e-03, 4.70239752e-09]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred, memory = full_forward_propagation(X.T, trained_nn, layer_dimensions)\n",
    "Y_pred.T[:5] # транспонираме отново изходния си продукт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отава да изчислим най-високата от трите стойности за всеки ред на Y_pred. Това става чрез np.argmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_categories = Y_pred.argmax(axis=0)\n",
    "Y_pred_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вече имаме по една стойност на ред, и формата вече ни напомня на разпределението в ирис. Остава само наложим правилните етикети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    setosa\n",
       "1    setosa\n",
       "2    setosa\n",
       "3    setosa\n",
       "4    setosa\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_predictions = pd.Series(Y_pred_categories).map(dict(zip([0,1,2],iris.species.unique())))\n",
    "structured_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека добавим предвидените стойности в рамката данни:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['predicted'] = structured_predictions\n",
    "iris['correct'] = iris.species == iris.predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     146\n",
       "False      4\n",
       "Name: correct, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_rate = 146/150\n",
    "success_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Успяхме да напишем и обучим невронна мрежа, която познава цветята от рода ирис с 97% точност! Поздравления!\n",
    "\n",
    "### Задача\n",
    "\n",
    "Използвайте кода, който написахме, за да обучите невронна мрежа за класифициране върху друг набор данни. (Започнете като измислите подходящи независими и зависими променливи!)\n",
    "\n",
    "Допълнителни задачи:\n",
    "\n",
    " - Ако имате повече опит с програмиране, опитайте се да превърнете невронната ни мрежа в обектно-ориентиран клас.\n",
    " - Експериментирайте с различни скорости на обучение и различни параметри за слоевете и вижте колко добре се справя мрежата ви в различни условия."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
